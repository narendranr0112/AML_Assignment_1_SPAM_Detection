{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "import"
      ],
      "metadata": {
        "id": "5ZsTGAfDz2cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaynzyHrz3LX",
        "outputId": "8bbcfb45-db9e-4244-ad7d-1f4a9c0b7e1b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lRKKlaCMzzz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK stopwords (if not already downloaded)\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load data from a given file path.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path (str): Path to the data file.\n",
        "\n",
        "    Returns:\n",
        "    - df (DataFrame): Pandas DataFrame containing the data.\n",
        "    \"\"\"\n",
        "    data_df = pd.read_csv(file_path)\n",
        "    return data_df\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text data.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): Text data to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "    - processed_text (str): Preprocessed text.\n",
        "    \"\"\"\n",
        "    # Converting to lowercase\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    # Removing special characters, numbers, and extra whitespaces\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "    # Joining the filtered words back into a string\n",
        "    processed_text = \" \".join(filtered_text)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Preprocess the data.\n",
        "\n",
        "    Parameters:\n",
        "    - data (DataFrame): Pandas DataFrame containing the data.\n",
        "\n",
        "    Returns:\n",
        "    - processed_data (DataFrame): Preprocessed Pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # Drop rows with missing values\n",
        "    processed_data = data.dropna()\n",
        "\n",
        "    # Apply text preprocessing to the \"text\" column\n",
        "    processed_data[\"processed_text\"] = processed_data[\"text\"].apply(preprocess_text)\n",
        "\n",
        "    # Drop duplicate texts\n",
        "    processed_data.drop_duplicates(\"processed_text\", inplace=True)\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def split_data(data):\n",
        "    \"\"\"\n",
        "    Split the data into train/validation/test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - data (DataFrame): Pandas DataFrame containing the data.\n",
        "\n",
        "    Returns:\n",
        "    - train_df (DataFrame): Train set.\n",
        "    - validation_df (DataFrame): Validation set.\n",
        "    - test_df (DataFrame): Test set.\n",
        "    \"\"\"\n",
        "    # Splitting the data into 80% training, 10% validation, and 10% test\n",
        "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "    validation_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
        "\n",
        "    return train_data, validation_data, test_data\n",
        "\n",
        "def store_splits(train_data, validation_data, test_data):\n",
        "    \"\"\"\n",
        "    Store the train/validation/test splits as CSV files.\n",
        "\n",
        "    Parameters:\n",
        "    - train_data (DataFrame): Train set.\n",
        "    - validation_data (DataFrame): Validation set.\n",
        "    - test_data (DataFrame): Test set.\n",
        "    \"\"\"\n",
        "    train_data.to_csv(\"train.csv\", index=False)\n",
        "    validation_data.to_csv(\"validation.csv\", index=False)\n",
        "    test_data.to_csv(\"test.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6KN1OZl0cPu",
        "outputId": "2373fb6c-4cd3-4bdf-8930-8bfa45d9987e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "file_path = \"/content/drive/MyDrive/emails.csv\"\n",
        "email_data = load_data(file_path)\n",
        "\n",
        "# Preprocess data\n",
        "preprocessed_data = preprocess_data(email_data)\n",
        "\n",
        "# Split and store data\n",
        "train_set, validation_set, test_set = split_data(preprocessed_data)\n",
        "store_splits(train_set, validation_set, test_set)\n"
      ],
      "metadata": {
        "id": "8WAwC1kH1gj5"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}